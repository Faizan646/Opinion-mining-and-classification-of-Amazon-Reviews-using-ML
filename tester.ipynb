{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"\" \n",
    "    text = re.sub(r\"<.*?>\", \"\", text) \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  \n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words] \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    clean_text = \" \".join(tokens) \n",
    "    return clean_text\n",
    "\n",
    "tokenizer = joblib.load(\"tokenizer.joblib\")\n",
    "\n",
    "# Load RNN model and tokenizer\n",
    "rnn_model = load_model(\"rnn_model.h5\")\n",
    "\n",
    "# Load CNN model and tokenizer\n",
    "cnn_model = load_model(\"cnn_model.h5\")\n",
    "\n",
    "# Load DNN model\n",
    "# dnn_model = load_model(\"dnn_model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define a function to evaluate and print classification report for each model\n",
    "# def evaluate_model(model, X_test, y_test, model_name):\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_pred_labels = [1 if pred.argmax() == 1 else 0 for pred in y_pred]\n",
    "#     y_test_labels = [1 if true.argmax() == 1 else 0 for true in y_test]\n",
    "#     report = classification_report(y_test_labels, y_pred_labels)\n",
    "#     print(\"\\n{} Model Classification Report:\".format(model_name))\n",
    "#     print(report)\n",
    "\n",
    "# # Evaluate and print classification report for each model\n",
    "# # evaluate_model(rnn_model, X_test, y_test, \"RNN\")\n",
    "# evaluate_model(cnn_model, X_test, y_test, \"CNN\")\n",
    "# # evaluate_model(dnn_model, X_test, y_test, \"DNN\")\n",
    "\n",
    "# def calculate_accuracy(model, X_test, y_test):\n",
    "#     _, accuracy = model.evaluate(X_test, y_test)\n",
    "#     return accuracy\n",
    "\n",
    "# # Calculate accuracies for each model\n",
    "# # rnn_accuracy = calculate_accuracy(rnn_model, X_test, y_test)\n",
    "# cnn_accuracy = calculate_accuracy(cnn_model, X_test, y_test)\n",
    "# # dnn_accuracy = calculate_accuracy(dnn_model, X_test, y_test)\n",
    "\n",
    "# # Print accuracies\n",
    "# # print(\"RNN Model Accuracy:\", rnn_accuracy)\n",
    "# print(\"CNN Model Accuracy:\", cnn_accuracy)\n",
    "# # print(\"DNN Model Accuracy:\", dnn_accuracy)\n",
    "\n",
    "# # Plot comparison line chart\n",
    "# # models = ['RNN', 'CNN', 'DNN']\n",
    "# # accuracies = [rnn_accuracy, cnn_accuracy, dnn_accuracy]\n",
    "# # plt.plot(models, accuracies, marker='o')\n",
    "# # plt.title('Model Comparison')\n",
    "# # plt.xlabel('Model')\n",
    "# # plt.ylabel('Accuracy')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, X_text):\n",
    "    predicted_sentiment = model.predict(X_text)[0]\n",
    "    print(\"predicted_sentiment:\", predicted_sentiment)\n",
    "    print(\" predicted_sentiment[0]:\",  predicted_sentiment.argmax())\n",
    "    if predicted_sentiment.argmax() == 0:\n",
    "        sentiment_label = \"negative\"\n",
    "    elif predicted_sentiment.argmax() == 1:\n",
    "        sentiment_label = \"positive\"\n",
    "    return sentiment_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_review = \"It is so quiet that you have to put your ear next to it to tell if it’s working. I love the lights on the top of the unit which alerts you to the air quality. I wasn’t sure if the sensor was working as the lights stayed blue. I set my oven to self clean and my unit which was two rooms away went from blue to green to orange to purple in seconds. Air speed went up to three. I moved the air purifier to the kitchen which cleaned the air in record time. I was so impressed that I ordered a second one for upstairs.\"\n",
    "\n",
    "bad_review = 'Seems to clean air, but the fan is noisy when on the lowest setting. To me it sounds like a barring rubbing sound. Thought it was broken so got a new one -- same problem. Rather disappointing. Own multiple filters but this manufacturer but this is the first one Im not happy with.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Check RNN Model Good Review\n",
    "cleaned_text = clean_text(good_review)\n",
    "X_text = tokenizer.texts_to_sequences([cleaned_text])\n",
    "try:\n",
    "    X_text = pad_sequences(X_text, maxlen=100)\n",
    "except:\n",
    "    X_text = pad_sequences(X_text, maxlen=200)\n",
    "    \n",
    "body_sentiment_rnn = predict_sentiment(rnn_model, tokenizer, X_text)\n",
    "print(body_sentiment_rnn)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check RNN Model Bad Review\n",
    "cleaned_text = clean_text(good_review)\n",
    "X_text = tokenizer.texts_to_sequences([cleaned_text])\n",
    "try:\n",
    "    X_text = pad_sequences(X_text, maxlen=100)\n",
    "except:\n",
    "    X_text = pad_sequences(X_text, maxlen=200)\n",
    "body_sentiment_rnn = predict_sentiment(rnn_model, tokenizer, X_text)\n",
    "print(body_sentiment_rnn)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check DNN Model Good Review\n",
    "cleaned_text = clean_text(good_review)\n",
    "X_text = tokenizer.texts_to_sequences([cleaned_text])\n",
    "try:\n",
    "    X_text = pad_sequences(X_text, maxlen=100)\n",
    "except:\n",
    "    X_text = pad_sequences(X_text, maxlen=200)\n",
    "body_sentiment_rnn = predict_sentiment(dnn_model, tokenizer, X_text)\n",
    "print(body_sentiment_rnn)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check DNN Model Bad Review\n",
    "cleaned_text = clean_text(good_review)\n",
    "X_text = tokenizer.texts_to_sequences([cleaned_text])\n",
    "try:\n",
    "    X_text = pad_sequences(X_text, maxlen=100)\n",
    "except:\n",
    "    X_text = pad_sequences(X_text, maxlen=200)\n",
    "body_sentiment_rnn = predict_sentiment(dnn_model, tokenizer, X_text)\n",
    "print(body_sentiment_rnn)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 122ms/step\n",
      "predicted_sentiment: [6.5859933e-03 9.9338335e-01 3.0728912e-05]\n",
      " predicted_sentiment[0]: 1\n",
      "body_sentiment_rnn: positive\n"
     ]
    }
   ],
   "source": [
    "# Check CNN Model Good Review\n",
    "cleaned_text = clean_text(good_review)\n",
    "X_text = tokenizer.texts_to_sequences([cleaned_text])\n",
    "X_text = pad_sequences(X_text, maxlen=200)\n",
    "body_sentiment_rnn = predict_sentiment(cnn_model, tokenizer, X_text)\n",
    "print(\"body_sentiment_rnn:\",body_sentiment_rnn)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 191ms/step\n",
      "predicted_sentiment: [9.9999917e-01 7.8400819e-07 4.6514978e-10]\n",
      " predicted_sentiment[0]: 0\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Check CNN Model Bad Review\n",
    "\n",
    "cleaned_text = clean_text(bad_review)\n",
    "X_text = tokenizer.texts_to_sequences([cleaned_text])\n",
    "X_text = pad_sequences(X_text, maxlen=200)\n",
    "body_sentiment_rnn = predict_sentiment(cnn_model, tokenizer, X_text)\n",
    "print(body_sentiment_rnn)\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
